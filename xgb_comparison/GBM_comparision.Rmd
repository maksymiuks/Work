---
title: "GBM_comparision"
author: "Szymon Maksymiuk"
date: "25 April 2019"
output: 
  html_document:
    toc: true  
    toc_float: true
    number_sections: true
---
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(kableExtra)
titanic_test_X <- select(read.csv("titanic_test.csv"), -c(survived))
titanic_test_Y <- select(read.csv("titanic_test.csv"), c(survived))
titanic_train_X <- select(read.csv("titanic_train.csv"), -c(survived))
titanic_train_Y <- select(read.csv("titanic_train.csv"), c(survived)) 
y <- unclass(titanic_test_Y)
y <- y$survived
```

# Introduction

Machine learning is changing very fast. During those years of constant development many algorithms were implemented in multiple programming languges. Since thsose implementations were made by different people, it was impossible to keep them identical. That is the reason why we may be worried about performance of our models acorss various programming languages. That is the reason why we are going to learn three gbm models, `gbm` R package, `h2o` implementation of gbm, and `scikit-learn` gbm in Python. Afterwards we are going to explain those models using `DALEX` library to inspect if they differ much. 

# Data

We are going to use preprocessed data from `titanic` dataseta avaiable in `DALEX` package. We split dataset into `titanic_test` and `titanic_train` that lack country column, in comparision to original `titanic`, all factor columns were one-hot-encoded and `survived` column was chcnged from two level factor `yes`, `no` to `0` and `1` numerci vector. In this vignette We want to classify if specified passager has survived Titanic's maiden voyage.


```{r}
kable(titanic_test_X %>% head(), "html") %>% kable_styling("striped") %>% 
  scroll_box(width = "100%")
kable(titanic_train_X %>% head(), "html") %>% kable_styling("striped") %>% 
  scroll_box(width = "100%")
```

#Models

## R gbm

First, we would like o introduce R implementation of `gbm` which we will acces thorugh `mlr` package. Specifying most of parameters helps us fiting almost exactly same models accros langugaes, at least in theory.

```{r message=FALSE, warning=FALSE}
set.seed(123, "L'Ecuyer")
library(mlr)

task <- makeClassifTask(id = "R", data = cbind(titanic_train_X, titanic_train_Y), 
                        target = "survived")
learner <- makeLearner("classif.gbm", 
                       par.vals = list(distribution = "bernoulli", n.trees = 5000,
                                       interaction.depth = 4, n.minobsinnode = 12, 
                                       shrinkage = 0.001, bag.fraction = 0.5, 
                                       train.fraction = 1), 
                       predict.type = "prob")

model_r <- train(learner, task)
performance(predict(model_r, newdata = cbind(titanic_test_X, titanic_test_Y)), 
            measures = auc)


```

As we can see R implementation of `gbm` was quite sucessfull. AUC measure above 80% shows high quality of our model, especially taking into account that our model is natural.

## h2o gbm

We will access `h2o` via R package. Using [h2o documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html) we are able to match as many parameters as it possible which will help objectively comapre models.

```{r warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
set.seed(123, "L'Ecuyer")
library(h2o)
h2o.init()
h2o.no_progress()
titanic_h2o <- as.h2o(cbind(titanic_train_X, titanic_train_Y))
titanic_h2o["survived"] <- as.factor(titanic_h2o["survived"])

titanic_test_h2o <- as.h2o(cbind(titanic_test_X, titanic_test_Y))
h2o_y <- as.h2o(as.factor(y))
  
```

```{r, warning=FALSE, message=FALSE}
set.seed(123, "L'Ecuyer")
model_h2o <- h2o.gbm(training_frame = titanic_h2o, y = "survived", 
                     distribution = "bernoulli", ntrees = 5000, max_depth = 4, 
                     min_rows =  12, learn_rate = 0.001)


h2o.auc(h2o.performance(model_h2o, newdata = titanic_test_h2o))




```

## scikit-learn gbm

Inspection of models that have been created at Python using R is not as hard as it may seem to. It is possbile thanks to two packages. `reticulate` avaiable at R and `pickle` from Python.

```{r}
library(reticulate)


model_py <- py_load_object("gbm.pkl", pickle = "pickle")
preds <- model_py$predict_proba(titanic_test_X)[,2]
mltools::auc_roc(preds, y)

```

`scikit-learn` turned up to be better than `h2o` and slightly better than R model. Fortunately high AUC measure , although it's importance is not the only thing that matter in models, therefore it's high time to make some explanations.


# Comparison

Because all three packages return sligthly diffrent objects, we have to specify predict functions in order to get plain predicted probabilities vector. 

```{r warning=FALSE, message=FALSE}
h2o_predict <- function(model, newdata){
                                          newdata_h2o <- as.h2o(newdata)
                                          res <- as.data.frame(h2o.predict(model, 
                                                                           newdata_h2o))
                                          return(as.numeric(res$p1))
                                          }
mlr_predict <- function(object, newdata) {
                                              pred <- predict(object, newdata=newdata)
                                              response <- pred$data[,1]
                                              return(response)
}

py_predict <- function(model, newdata){
                                          model$predict_proba(newdata)[,2]
                                        }

library(DALEX)
r_explain <- DALEX::explain(model_r, data = titanic_test_X,
                            y = y, label = "R",
                            predict_function = mlr_predict)


h2o_explain <- DALEX::explain(model_h2o, data = titanic_test_X,
                              y = y, label = "h2o",
                              predict_function = h2o_predict)


py_explain <- explain(model_py, data = titanic_test_X,
                      y = y, label = "python",
                      predict_function = py_predict)


```

## Model performance

With explainers ready, we can compare our models in order to find possible differences. Models performance and residual distribution gets our first look.

```{r}
plot(model_performance(r_explain), model_performance(h2o_explain), model_performance(py_explain))
```
As we can see, models are quite similiar. The biggest difference is residuals distrubution in [0, 0.13] compartment where R `gbm` has more observation. R is worse in compartment [0.13, 0.25] aswell, but diffence is not as easy to spot as in previous area. 

## Variable importance

```{r}
plot(variable_importance(r_explain), variable_importance(h2o_explain), variable_importance(py_explain))
```

This time we can see significant difference. `h2o` model figured out correlation between `gender.male` and `gender.female` and dropped one of them. Other models use both of those columns. What is interesting, next four most significant variables are the same for all three of models. 

## pdp plots

```{r}
pdp_r  <- variable_response(r_explain, variable = "fare", type = "pdp")
pdp_h2o  <- variable_response(h2o_explain, variable = "fare", type = "pdp")
pdp_py <- variable_response(py_explain, variable = "fare", type = "pdp")
plot(pdp_r, pdp_h2o, pdp_py)

```
